AWS LAMBDA 1 - python - an img added -S3

Create an S3 bucket
Create lambda function
 - use blueprint
 - Get S3 obj in python option
 - create a new role
 - select  s3 bicket 
 - Event type - allobject create events
Make changes in code
Upload obj in S3
Monitor in Lambda and oprn CloudWatch Log

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

AWS Lambda 2 - Dynamo Db 

Create Dynamo Table
 - enter patition key and sort key -(value)
 - add items in table

(Role already created)
Create IAM role with dynamo full access 
 - service lambda
 - search dynao - full access

Create Lambda function
 - blueprint use karele - dynamo python
 - add role existing 
Write this code:::

import boto3
import json

dynamodb_client = boto3.resource('dynamodb')
table = dynamodb_client.Table('darshiltable1')

def lambda_handler(event,context):
    try:
        response = table.put_item(Item=event)
        return table.scan()
    except:
        raise



Test - change json
Save and test and seeeeee


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

S3 host website
- predesigned ooorrrr
 - Do not block public access while creating bucket
 - Enable static website hosting
 - change bucketpolicy - generator - * - getObject
 - in json add /*


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::bucket1thadomal/*"
        }
    ]
}


acl me change when pipeline
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=


Sonarqube
- Sonarqube
- bin path in system variable
- wondows folder -- cmd StartSonar.bat



-=-=-=-=-=-=-==--=-=-=-=-=-=-==--=-=-=-=-=-=-==--=-=-=-=-=-=-==--=-=-=-=-=-=-==-
Pipeline

Create Bucket
ACL enabled
Object Writer
Uncheck block all public access
Create Buucket

CodePipeline
Create
- New serverice role or existing
- Source Provider - GithubVersion1 
Connect to github
Confirm 
Repo
Branch
Change detection options - Github Webhooks


build Provider - AWS CodeBuild
Region same as bucket
CLICK ON ***Create project
 - Managed Image
 -OS - Ubuntu
Runtime - Standard
Image - 7.0
Service role - you know
Use buildspec file
untick cloudwatch


Build Type - single Build
Deploy Provider -S3
Bucket 
No object key
Instead Extract the file before deploy


Create pipelineeee

SUCCEEDS ALL 3 steps
Click on deplyed s3

Permission policy


-=-=-=-=-=-=-=-=---=-=-=-=--=-=-=-==-=-=-=-
Terrraform

terraform { 
    required_providers {
        aws = {
            source = "hashicorp/aws" 
            version = "~> 4.0"
        }
    }
}

provider "aws" {
    region = "ap-south-1"
    access_key="AKIAZZJD4A7Z73CCUOW5"
    secret_key="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"
}

resource "aws_instance" "public_instance" {
    ami           = "ami-0287a05f0ef0e9d9a" 
    instance_type = "t2.micro"
}


././././.
Mine


provider "aws" {	region = "us-east-1"	access_key = ""	secret_key = ""}resource "aws_instance" "Ubuntu" {	ami = ""	instance_type = "t2.micro"}

./././.
S3
provider ka same
resource "aws_s3_bucket" "my_bucket" {
bucket = "darshilbbbb2343"
}

